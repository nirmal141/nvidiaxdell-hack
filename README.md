# Sentio

> **Visual RAG for Security & Law Enforcement** ‚Äî An AI-powered video intelligence platform that transforms raw surveillance footage into searchable, queryable knowledge.

![Python](https://img.shields.io/badge/Python-3.12+-blue)
![FastAPI](https://img.shields.io/badge/FastAPI-0.100+-green)
![NVIDIA](https://img.shields.io/badge/NVIDIA-NIM%20%2B%20Local%20GPU-76B900)
![License](https://img.shields.io/badge/License-MIT-yellow)

---

## üéØ What is Sentio?

Sentio is a **Visual Retrieval-Augmented Generation (RAG)** system designed for security and law enforcement applications. It processes video footage (body cams, surveillance, dashcams) and enables natural language search across:

- **Visual content** ‚Äî What's happening in frames
- **Spoken audio** ‚Äî What people are saying (transcribed with timestamps)
- **Detected objects** ‚Äî People, vehicles, fire, weapons

### Key Differentiators

| Traditional Approach | Sentio Approach |
|---------------------|-----------------|
| Manual video review | AI-powered semantic search |
| Keyword-based search | Natural language queries |
| Single video at a time | Cross-video intelligence |
| Visual only | Visual + Audio fusion |
| Cloud-dependent | Hybrid Cloud + Local GPU |

---

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                              SENTIO PLATFORM                              ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ   React     ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ                 FastAPI Backend                  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ   Frontend  ‚îÇ    ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ ‚îÇ
‚îÇ  ‚îÇ             ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÇ  ‚îÇ            Video Processing Pipeline        ‚îÇ ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ ‚îÇ ‚îÇ
‚îÇ                     ‚îÇ  ‚îÇ  ‚îÇ  Frame  ‚îÇ  ‚îÇ  Audio  ‚îÇ  ‚îÇ   Object    ‚îÇ  ‚îÇ ‚îÇ ‚îÇ
‚îÇ                     ‚îÇ  ‚îÇ  ‚îÇ  VLM    ‚îÇ  ‚îÇ Whisper ‚îÇ  ‚îÇ  Detection  ‚îÇ  ‚îÇ ‚îÇ ‚îÇ
‚îÇ                     ‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ ‚îÇ ‚îÇ
‚îÇ                     ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îÇ
‚îÇ                     ‚îÇ                       ‚îÇ                          ‚îÇ ‚îÇ
‚îÇ                     ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ ‚îÇ
‚îÇ                     ‚îÇ  ‚îÇ           Embedding + Vector Store           ‚îÇ‚îÇ ‚îÇ
‚îÇ                     ‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ‚îÇ ‚îÇ
‚îÇ                     ‚îÇ  ‚îÇ  ‚îÇ  NV-Embed   ‚îÇ    ‚îÇ    Milvus Lite      ‚îÇ  ‚îÇ‚îÇ ‚îÇ
‚îÇ                     ‚îÇ  ‚îÇ  ‚îÇ  (384-dim)  ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  (Local Vector DB)  ‚îÇ  ‚îÇ‚îÇ ‚îÇ
‚îÇ                     ‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ‚îÇ ‚îÇ
‚îÇ                     ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ ‚îÇ
‚îÇ                     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                                                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ
‚îÇ  ‚îÇ                         AI Model Stack                               ‚îÇ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ LLaVA 1.5 7B    ‚îÇ  ‚îÇ Whisper Base    ‚îÇ  ‚îÇ YOLOv8x + SAM2      ‚îÇ  ‚îÇ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ (Local GPU VLM) ‚îÇ  ‚îÇ (Audio‚ÜíText)    ‚îÇ  ‚îÇ (Detection+Segment) ‚îÇ  ‚îÇ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ              NVIDIA NIM Cloud (Fallback / Answer Gen)           ‚îÇ‚îÇ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ    Llama 3.2 90B Vision  ‚îÇ  Llama 3.1 70B  ‚îÇ  NV-EmbedQA E5 V5  ‚îÇ‚îÇ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üõ†Ô∏è Technology Stack

### Core AI Models

| Component | Model | Why This Choice |
|-----------|-------|-----------------|
| **Vision-Language** | LLaVA 1.5 7B (Local) | Fast local inference, good quality descriptions, runs on 8GB VRAM |
| **Audio Transcription** | Whisper Base (faster-whisper) | Best OSS speech recognition, word-level timestamps, VAD filtering |
| **Object Detection** | YOLOv8x | State-of-the-art detection, 80+ classes, real-time on GPU |
| **Segmentation** | SAM2 (Segment Anything 2) | Click-to-track any object, zero-shot segmentation |
| **Embeddings** | Sentence-Transformers (Local) | 384-dim vectors, fast local embedding |
| **Answer Generation** | Llama 3.1 70B (NIM Cloud) | High-quality reasoning with context |

### Why Local + Cloud Hybrid?

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    COST & LATENCY OPTIMIZATION                  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                 ‚îÇ
‚îÇ  LOCAL GPU (Most Operations)         CLOUD API (When Needed)   ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Frame descriptions (LLaVA)      ‚îú‚îÄ‚îÄ Complex Q&A (Llama)  ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Audio transcription (Whisper)   ‚îî‚îÄ‚îÄ High-stakes analysis ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Object detection (YOLO)                                   ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Segmentation (SAM2)                                       ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ Embeddings (sentence-transformers)                        ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  üí∞ Cost: $0 per frame              üí∞ Cost: ~$0.001 per query ‚îÇ
‚îÇ  ‚ö° Latency: ~200ms                 ‚ö° Latency: ~2-3s           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Infrastructure

| Component | Technology | Purpose |
|-----------|------------|---------|
| **Backend** | FastAPI | Async REST API + WebSocket for real-time updates |
| **Frontend** | React + Tailwind | Modern dashboard with database-style video library |
| **Vector DB** | Milvus Lite | Embedded vector search, no external dependencies |
| **Video Processing** | OpenCV + CUDA | GPU-accelerated frame extraction |
| **GPU Runtime** | NVIDIA NGC Container | Pre-configured PyTorch + CUDA environment |

---

## ‚ú® Key Features

### 1. Multi-Modal Search
Search across **visual content** AND **spoken audio** simultaneously.

```
Query: "when did the officer mention license"
‚îî‚îÄ‚îÄ Searches both frame descriptions AND audio transcriptions
    ‚îî‚îÄ‚îÄ Returns: [AUDIO] "License and registration please" @ 01:23
```

### 2. Smart Deduplication
Results are grouped by **30-second windows** ‚Äî no more seeing 20 nearly-identical frames.

```
Before: Frame@1:14, Frame@1:15, Frame@1:16, Frame@1:17...
After:  Best match from 1:00-1:30 window, Best match from 1:30-2:00 window...
```

### 3. Real-Time Object Detection
Click "Detect" to identify all objects in current frame:
- People count
- Vehicle detection (cars, trucks, motorcycles)
- Fire detection (custom color analysis)
- Weapons, bags, electronics

### 4. Click-to-Track (SAM2)
Click on any object ‚Üí AI segments and tracks it across frames.

### 5. AI-Powered Summaries
Every search returns an AI analysis synthesizing findings across videos:

```
"There are multiple car accidents in the footage. In [Florida Man] at [02:16], 
a car has crashed into a tree. In the same video at [01:14], a person is lying 
on the ground nearby..."
```

---

## üìÅ Project Structure

```
nirmal-hackathon/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ main.py                    # FastAPI entry point
‚îÇ   ‚îú‚îÄ‚îÄ config.py                  # Configuration management
‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ routes.py              # All REST + WebSocket endpoints
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ qa_service.py          # Core Q&A orchestration
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ video_processor.py     # Frame extraction + thumbnails
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ vector_store.py        # Milvus vector operations
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ local_vlm.py           # LLaVA local inference
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ local_embedding.py     # Sentence-transformers embeddings
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ audio_transcriber.py   # Whisper transcription
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ object_detector.py     # YOLO + fire detection
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sam2_tracker.py        # Click-to-segment
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ nim_client.py          # NVIDIA NIM Cloud APIs
‚îÇ   ‚îî‚îÄ‚îÄ models/
‚îÇ       ‚îî‚îÄ‚îÄ schemas.py             # Pydantic models
‚îú‚îÄ‚îÄ frontend/
‚îÇ   ‚îî‚îÄ‚îÄ src/
‚îÇ       ‚îú‚îÄ‚îÄ App.tsx                # Main application
‚îÇ       ‚îú‚îÄ‚îÄ components/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ Dashboard.tsx      # Database-style video library
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ VideoPlayer.tsx    # Video + detection overlay
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ Chat.tsx           # Q&A interface
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ Sidebar.tsx        # Video list + upload
‚îÇ       ‚îî‚îÄ‚îÄ api.ts                 # API client
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ videos/                    # Uploaded videos + metadata
‚îÇ   ‚îî‚îÄ‚îÄ milvus/                    # Vector database files
‚îú‚îÄ‚îÄ requirements.txt               # Python dependencies
‚îú‚îÄ‚îÄ run.py                         # Development server
‚îî‚îÄ‚îÄ register_videos.py             # Bulk video registration
```

---

## üöÄ Quick Start

### Prerequisites
- Python 3.12+
- NVIDIA GPU (8GB+ VRAM recommended)
- Docker (for GPU container)
- NVIDIA API key from [build.nvidia.com](https://build.nvidia.com)

### Option 1: Docker (Recommended)

```bash
# Run NVIDIA container with GPU
docker run --gpus all -it --rm \
  -v $(pwd):/workspace \
  -v ~/hf-models:/models \
  -p 8080:8080 \
  -w /workspace \
  nvcr.io/nvidia/pytorch:25.11-py3

# Inside container
pip install -r requirements.txt
pip install faster-whisper
apt-get update && apt-get install -y ffmpeg
echo "NVIDIA_API_KEY=nvapi-xxx" > .env
python run.py
```

### Option 2: Local venv

```bash
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
echo "NVIDIA_API_KEY=nvapi-xxx" > .env
python run.py
```

### Access
Open [http://localhost:8080](http://localhost:8080)

---

## üìã API Reference

### Video Management
| Method | Endpoint | Description |
|--------|----------|-------------|
| `GET` | `/api/videos` | List all videos |
| `POST` | `/api/videos/upload` | Upload video file |
| `DELETE` | `/api/videos/{id}` | Delete video + data |
| `GET` | `/api/videos/{id}/stream` | Stream video |
| `GET` | `/api/videos/{id}/thumbnail` | Get thumbnail |

### AI Processing
| Method | Endpoint | Description |
|--------|----------|-------------|
| `POST` | `/api/videos/{id}/process` | Start AI analysis |
| `POST` | `/api/videos/{id}/stop` | Stop processing |
| `GET` | `/api/videos/{id}/status` | Get progress |
| `WS` | `/ws/progress/{id}` | Real-time progress |

### Search & Q&A
| Method | Endpoint | Description |
|--------|----------|-------------|
| `POST` | `/api/search` | Global semantic search |
| `POST` | `/api/videos/{id}/ask` | Ask question about video |

### Detection & Tracking
| Method | Endpoint | Description |
|--------|----------|-------------|
| `POST` | `/api/videos/{id}/detect` | Detect objects at timestamp |
| `POST` | `/api/videos/{id}/segment` | Segment object at click point |

---

## üîß Configuration

### Environment Variables

```bash
NVIDIA_API_KEY=nvapi-xxx          # Required for cloud LLM
CACHE_DIR=/models                  # Model cache directory
WHISPER_MODEL=base                 # tiny/base/small/medium
```

### Config Options (`app/config.py`)

| Setting | Default | Description |
|---------|---------|-------------|
| `use_local_vlm` | `True` | Use local LLaVA vs cloud |
| `use_local_embedding` | `True` | Use local embeddings vs cloud |
| `frame_sample_interval` | `1.0` | Seconds between frame samples |
| `embedding_dim` | `384` | Vector dimension (384 local, 1024 cloud) |

---

## üìä Why Sentio is Better

### vs. Traditional Video Search
| Aspect | Traditional | Sentio |
|--------|-------------|--------|
| Search method | Keyword/metadata | Semantic understanding |
| Audio | Ignored | Transcribed + searchable |
| Cross-video | Manual | Automatic |
| Evidence linking | Manual | AI-assisted with timestamps |

### vs. Cloud-Only Solutions
| Aspect | Cloud-Only | Sentio |
|--------|------------|--------|
| Cost per video | $1-10 | ~$0.10 (mostly local) |
| Data privacy | Leaves device | Stays local |
| Latency | 2-3s/frame | ~200ms/frame |
| Offline support | None | Full (except Q&A) |

### vs. Basic Vision AI
| Aspect | Basic Vision | Sentio |
|--------|--------------|--------|
| Audio | ‚ùå | ‚úÖ Whisper transcription |
| Object tracking | ‚ùå | ‚úÖ SAM2 click-to-track |
| Temporal context | ‚ùå | ‚úÖ 30s window deduplication |
| Answer generation | ‚ùå | ‚úÖ LLM with sources |

---

## üîí Security & Privacy

- **Local-first processing** ‚Äî Video analysis runs on your GPU
- **No cloud upload** ‚Äî Only text queries go to cloud API (optionally)
- **Air-gap capable** ‚Äî Can run fully offline with local models
- **Evidence integrity** ‚Äî Original files never modified

---

## üìù License

MIT License ‚Äî See [LICENSE](LICENSE) for details.

---

## üôè Acknowledgments

- **NVIDIA** ‚Äî NIM Cloud APIs, NGC Containers
- **Meta** ‚Äî LLaVA, Llama, SAM2 models
- **OpenAI** ‚Äî Whisper architecture
- **Ultralytics** ‚Äî YOLOv8
- **Milvus** ‚Äî Vector database
- **FastAPI** ‚Äî Web framework

---

<div align="center">

**Built for the NVIDIA AI Hackathon 2024**

[Demo](http://localhost:8080) ¬∑ [Report Bug](https://github.com/yourrepo/issues) ¬∑ [Request Feature](https://github.com/yourrepo/issues)

</div>
